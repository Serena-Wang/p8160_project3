---
title: "task 2"
date: "2023-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(extraDistr)
library(MASS)
library(progress)
library(foreach)
library(doParallel)
```

As the day of year $x_{i,1}(t)$ and time $t$ are two variables in given model, we need to transform `time` into some suitable formats. Normally, records are taken every 6 hour at 00:00:00, 06:00:00, 12:00:00 and 18:00:00. However, some records are taken at other time points. These records are ineffective as there are no record 6 hour before or after those time points to help train or test given models. Thus, we remove those observations
```{r echo=FALSE}
dt = read.csv("/Users/yujia/Downloads/hurrican703.csv") %>%
  janitor::clean_names() %>% 
  rename(year = season) %>% 
  separate(time, into = c("date", "hour"), sep = " ") %>% 
  mutate(
    date = str_remove(date, "\\("),
    hour = str_remove(hour, "\\)")
  ) %>% 
  mutate(month = str_match(date, "-\\s*(.*?)\\s*-")) %>% 
  mutate(month = gsub("[-]", "", month)) %>% 
  filter(hour == "00:00:00" | hour == "06:00:00" | hour == "12:00:00" | hour == "18:00:00") %>% 
  mutate(
    hour = str_replace(hour, ":00:00", ""),
    hour = as.numeric(hour),
    month = month[,1],
    month = as.numeric(month),
    nature = as.numeric(as.factor(nature))
  ) %>%
  mutate(lat_change = c(NA, diff(latitude)),
         lng_change = c(NA, diff(longitude)),
         wind_change = c(NA, diff(wind_kt)))
```

Then, we randomly partition the data into 80:20 to train the model and test the model performance:
```{r echo=FALSE}
set.seed(1)
id = unique(dt$id)
num_id = length(id)
train_id = sample(id, 0.8*num_id)
train_data = dt[which(dt$id %in% train_id),]
```


With the conditional posterior distributions calculated, we can utilized Gibbs sampling MCMC algorithm:
```{r Initialize parameters}
#beta matrix
beta_i <- function(dt, mu, sigma2, Sigma){
   n = length(dt)
   B = NULL
   for (i in 1:n){
       # stuffs to define the distribution
       X = as.matrix(dt[[i]][,-1]) 
       y = as.vector(dt[[i]][,1])
       V = sigma2^(-1) * t(X) %*% X + solve(Sigma)
       M = sigma2^(-1) * t(y) %*% X + muvec %*% solve(Sigma)
       vcov_bi = solve(V)
       mean_bi = vcov_bi %*% t(M) 
       bi = mvrnorm(1, mu = mean_bi, Sigma = vcov_bi)
       B = rbind(B, bi)
   }
   return(as.matrix(B))
}

#Sigma by inverse wishart dist
Sigma <- function(B, mu){
    n = nrow(B)
    S = diag(0, ncol(B), ncol(B))
    for (i in 1:n){
        S = S + (B[i,] - mu) %*% t(B[i,] - mu)
    }
    omega = S + diag(1, ncol(B), ncol(B)) #scale matrix
    Sigma = rinvwishart(1, nu = n, Omega = omega, checkSymmetry = F)
    return(Sigma)
}

#mu 
mu <- function(B, Sigma){
    n = nrow(B)
    col_mu = colMeans(B)
    mu = mvrnorm(1, col_mu, Sigma/n)
    return(mu)
}

#sigma square 
sigma2 <- function(dt, B) {
  alpha = nrow(dt)/2
  beta = beta_gamma(dt, B)
  sigmasq = extraDistr::rinvgamma(1, alpha = alpha, beta = beta) 
  return(sigmasq)
}

```


```{r}
#Gibbs
# Function to compute log likelihood of data given parameters
loglik <- function(dt, beta, mu, sigma2) {
  n <- length(dt)
  loglik <- 0
  for (i in 1:n) {
    y <- dt[[i]][, 1]
    X <- as.matrix(dt[[i]][,-1])
    loglik <- loglik + dmvnorm(y, mean = X %*% beta[i,], sigma = sqrt(sigma2) * diag(nrow(y)), log = TRUE)
  }
  return(sum(loglik))
}

# Function to run MCMC algorithm
run_mcmc <- function(dt, n_iter, burn_in) {
  n <- length(dt)
  p <- ncol(dt[[1]]) - 1
  
  # Initialize parameters
  beta <- matrix(rnorm(n * p), nrow = n)
  mu <- rep(0, p)
  Sigma <- diag(p)
  sigma2 <- 1
  
  # Create matrix to store samples
  beta_samples <- matrix(0, nrow = n_iter, ncol = n * p)
  mu_samples <- matrix(0, nrow = n_iter, ncol = p)
  Sigma_samples <- array(0, dim = c(p, p, n_iter))
  sigma2_samples <- rep(0, n_iter)
  loglik_samples <- rep(0, n_iter)
  
  for (i in 1:n_iter) {
    
    # Update beta_i
    beta <- beta_i(dt, mu, sigma2, Sigma)
    
    # Update mu
    mu <- mu(beta, Sigma)
    
    # Update Sigma
    Sigma <- Sigma(beta, mu)
    
    # Update sigma2
    sigma2 <- sigma2(dt, beta)
    
    # Save samples after burn-in
    if (i > burn_in) {
      beta_samples[i - burn_in,] <- as.vector(beta)
      mu_samples[i - burn_in,] <- mu
      Sigma_samples[,,i - burn_in] <- Sigma
      sigma2_samples[i - burn_in] <- sigma2
      loglik_samples[i - burn_in] <- loglik(dt, beta, mu, sigma2)
    }
  }
  
  return(list(beta = beta_samples, mu = mu_samples, Sigma = Sigma_samples, sigma2 = sigma2_samples, loglik = loglik_samples))
}

```


```{r}
# Check for convergence with autocorrelation plots
library(coda)

# Extract MCMC samples from list
mcmc_list <- run_mcmc(dt, n_iter = 5000, burn_in = 1000)
beta_samples <- mcmc_list$beta

```








