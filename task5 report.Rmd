---
title: "P8160 - Baysian modeling of hurrican trajectories"
author: "Jiahao Fan, Tianwei Zhao, Yijin Wang, Youlan Shen, Yujia Li"
date: "2023-05-02"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{amsthm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
library(knitr)
library(tibble)
```

\newpage 
# Background and Objectives
As hurricanes affect people, the ability to forecast hurricanes is essential for minimizing the risks in suffered areas. Hereby, a hierarchical Bayesian strategy for modeling North Atlantic hurricane counts since 1950 is illustrated. Model integration would be expected to achieve through a Markov chain Monte Carlo algorithm. Contingent on the availability of values for the covariates, the models can be used to explore the seasonality among track data and make predictive inferences on hurricanes.

The data given has 703 observations and following features are recorded for each hurricanes in the North Atlantic: 

* ID: ID of the hurricanes

* Season: In which the hurricane occurred

* Month: In which the hurricane occurred

* Nature: Nature of the hurricane (ET-Extra tropical;DS-Disturbance;NR-Not rated;SS-Sub tropical; TS-Tropical storm)

* time: dates and time of the record

* Latitude and Longitude: The location of a hurricane check point

* Wind.kt: Maximum wind speed (in Knot) at each check point


# Methods
## Baysian Model  
## MCMC Algorithm 

Given the fact that we cannot directly sample from the joint posterior distribution calculated above. We choose to use Gibbs sampling. Gibbs sampling is a sequential sampler that samples from the conditional distribution of each parameter so that the final collection of samples forms a Markov chain whose stationary distribution is the joint distribution that we are interested in. 

In our case, conditional posteriors are more manageable than the joint posterior. Conditional posteriors for each parameter are calculated as the following:

!!! ADD CONDITONAL POSTERIOR

Still, we could not find a regular distribution that has the density function that matches with the conditional posterior of $\sigma^2$. Therefore, we incorporated Metropolis Hasting Random Walk into Gibbs sampling. Metropolis Hasting Random Walk finds approximated values of $\sigma^2$ by accepting and rejecting proposed value with an acceptance probability that helps maintain the detailed balance condition. In our algorithm, we work with log-scaled posterior values instead of the original-scaled posterior because $\frac{1}{\sigma^N}$ dominates the entire posterior value where $N$ is the total number of observations in the data set.

In our investigation of hurricane wind speed, we used Gibbs sampling and Metropolis Hasting Random Walk as the following:

![Gibbs Sampling](algo_fig/gibbs_sampling.png)

![Metropolis Hasting Random Walk](algo_fig/random_walk.png)

\newpage


We decide starting values of each parameters by fitting a linear mixed effects model that is the same as the proposed model in a frequentist way and then extract its corresponding coefficients. Starting values for $\gamma$ are the fixed effects coefficients in the linear mixed effects model.

```{r table1, echo = FALSE, message = FALSE, wanrings = FALSE}
starting_val_df <- tibble(
  terms = c("$\\gamma$", "$\\beta_{i_{1},...,i_{4}}$","$\\sigma^2$", "$\\beta_0$", "$\\mu$", 
            "$A = \\Sigma^{-1}$"
            
                    ),
  starting_val = c("Fixed effect coefficients in the fitted Linear Mixed Effects Model",
                    "Random effect coefficients in the fitted Linear Mixed Effects Model",
                    "Residual Variance in the fitted Linear Mixed Effects Model",
                    "Sum of random effect and fixed effect intercepts in the fitted Linear Mixed Effects Model",
                   "Mean of the $\\beta_{i_{1},...,i_{n}}$ for each predictor",
                   "Mean of the Wishart distribution with $\\nu$ = 5 and scale matrix with 0.7 on the main diagnoal and 0.2 on off-diagnoal"
                    )
)
kable(starting_val_df,
      col.names = c("Terms", "Starting Value"),
      caption = "Starting Values for Gibbs Sampling")
```


With the stating values above, we assume that the true $\sigma^2$ will be around 27 and tried a few values for window size $a$. Finally, with $a = 0.8$, $\sigma^2$ samples converge pretty quickly and the acceptance rate is around 46% with 500 iterations. Therefore, we set $a = 0.8$ in each iteration of Gibbs Sampling and burn the first 400 samples of $\sigma^2$. 

Although our algorithm is not the most efficient algorithm, it has two main advantages: 1) It is simple to compute and does not rely on the calculation of gradient of log densities as Hamiltonian Monte Carlo; 2) By incorporating the Random Walk, we have the flexibility to control the range of sampling area when the conditional distribution of parameter of interest is difficult to simulate from and when our initial prior is not informative .  

## MCMC Results

We ran our algorithm with these starting values for 6000 iterations and decided to burn the first 3000 samples for each parameters for analysis in the rest of this project. We use diagnostic plots to evaluate the quality of these samples. We created trace plots for all 6000 samples for each parameter, and autocorrelation plots and histogram plots for the second half of the samples after burn-in. 

For a good set of samples, we expect to see the chain in trace plot stablize after a short amount of iterations. After the burn-in, we expect to see small or no autocorrelation and somewhat normal histograms. 

Overall, MCMC samples for the fixed effect parameters $\gamma$ converge pretty well and their histograms have a nice bell shpae curve. There is very small autocorellation except $\gamma_{10}$ which is the coefficient for the Season variable.

On the other hand, MCMC samples for the random effects parameters have more oscilliations in the trace plots even though most of them converge. For $\beta_{i_{0},...,i_{4}}$ and $\mu$, post burn-in samples have somewhat normal histograms and their autocorrelations decrease to a tolerable range of values within 50 lags. Samples for the inverse of the covariance matrix for $\beta_{i_{0},...,i_{4}}$ are tricky to converge. By the design of covariance matrix, if the samples of a variance term have too much oscilliations, the osciliations will propagate to other terms in the covairiance matrix as well. But still, a good number of terms in the inverse of the covariance matrix have converging samples that have small autocorrelation.

Finally, for $\sigma^2$, our random walk is doing a good job. Samples do converge and have small correlation.

![Trace plots and ACF plots for $\beta$](mcmc_results_fig/transf_yr/beta/beta_chain_acf.png)

![Histograms for $\beta$](mcmc_results_fig/transf_yr/beta/beta_histograms.png)

![Trace plots and ACF plots for $\mu$](mcmc_results_fig/transf_yr/mu/mu_chain_acf.png)

![Histograms for $\mu$](mcmc_results_fig/transf_yr/mu/mu_histograms.png)

![Trace plots for $A = \Sigma^{-1}$](mcmc_results_fig/transf_yr/A/A_chain.png)

![ACF plots for $A = \Sigma^{-1}$](mcmc_results_fig/transf_yr/A/A_acf.png)

![Histograms for $A = \Sigma^{-1}$](mcmc_results_fig/transf_yr/A/A_histograms.png)

![Trace plots for $\gamma$](mcmc_results_fig/transf_yr/gamma/gamma_chains.png)

![ACF plots for $\gamma$](mcmc_results_fig/transf_yr/gamma/gamma_acf.png)

![Histograms for $\gamma$](mcmc_results_fig/transf_yr/gamma/gamma_histograms.png)

![Histograms for $\sigma^2$](mcmc_results_fig/transf_yr/sigma_squared/sigma_squared.png)

\newpage

# Results
## Seasonal Analysis 
## Prediction 
Accurately predicting hurricane wind speeds is crucial in hurricane forecasting. To achieve this, we calculated the posterior means of the parameters by averaging the last three thousand values in each MCMC chain, and used these means to predict wind speeds at each time point. The estimated results of parameters are shown below. 
\begin{figure}[H] 
\centering
\includegraphics[width=0.5\textwidth]{Results/Fix.png} 
\caption{Fixed Effects Gamma Estimate for Each Covariate}
\label{Table}
\end{figure}

\begin{figure}[H] 
\centering
\includegraphics[width=0.5\textwidth]{Results/Random.png} 
\caption{Random Effects Beta Estimated for Each Hurriane}
\label{Table}
\end{figure}

To assess the performance of our Bayesian model, we used the Root Mean Square Error (RMSE) of the predicted wind speeds compared to actual wind speeds for several hurricanes. In the table on the left, we list the 10 hurricanes with the least RMSE, with the first hurricane having only six observations but still achieving a small weighted average error of 1.041. This result indicates that our model can make accurate predictions even for hurricanes with fewer observations. 
\begin{figure}[H] 
\centering
\includegraphics[width=0.5\textwidth]{Results/RMSE.png} 
\caption{Prediction Standard Error of Wind Speed in Ascending Order}
\label{Table}
\end{figure}

We further analyzed the model's performance on hurricanes with more than 50 observations and those with fewer than 50 observations, labeled 'more observations' and 'fewer observations', respectively, as shown in the graph on the right. The average RMSE for both categories is around 5, suggesting that our model's accuracy is not significantly affected by the amount of data available. This finding is reassuring and indicates that our model is effective in predicting wind speeds for hurricanes with varying amounts of data.
\begin{figure}[H] 
\centering
\includegraphics[width=0.5\textwidth]{Results/MoreFewer.png} 
\caption{Prediction Standard Error of Wind Speed by Number of Observation}
\label{Figure}
\end{figure}

Additional plots are presented below to further investigate the model's performance. The graph displaying the average prediction points by month shows that our model performs well across different months. When we evaluate our predictions by year, we also see consistent performance across different years and types of hurricanes.
\begin{figure}[H] 
\centering
\includegraphics[width=1.0\textwidth]{Results/MONTH.png}
\caption{Actual Wind Speed vs.Predicted Wind Speed by Month}
\label{Figure}
\end{figure}

\begin{figure}[H] 
\centering
\includegraphics[width=1.0\textwidth]{Results/YEAR.png} 
\caption{Actual Wind Speed vs.Predicted Wind Speed by Year}
\label{Figure}
\end{figure}

In conclusion, our analysis using RMSE and additional graphs demonstrates that our Bayesian model is capable of making accurate predictions for hurricane wind speeds. These results suggest that our model is effective in predicting wind speeds for hurricanes with varying amounts of data, and performs consistently across different months, years, and types of hurricanes. These findings have significant implications for the field of hurricane prediction and can inform future research in this area.

# Discussion 
In summary, our application of the MCMC algorithm to the hurricane Bayesian model has yielded valuable insights into the prediction of hurricane wind speeds. Our analysis revealed significant seasonal differences in wind speeds and did not find any evidence to support the claim of increasing wind speeds over the years. These findings emphasize the importance of accounting for seasonal differences when modeling hurricane wind speeds. Although our final model did not achieve perfect convergence for every parameter, we were still able to produce reliable predictions. The use of MCMC has allowed us to overcome the challenges posed by the complicated distribution and high-dimensional space of hurricane wind speed data, demonstrating the usefulness of this algorithm in Bayesian analysis. 

MCMC is a powerful tool in Bayesian statistics that is particularly useful for handling complicated distributions in high-dimensional spaces. One of its main advantages is its ability to explore the parameter space efficiently and estimate the posterior distribution of model parameters. This is particularly helpful when direct sampling is practically impossible in high-dimensional spaces.

Despite its many benefits, MCMC has some drawbacks that need to be addressed. One major issue is the assumption of prior distribution, which can introduce bias and affect the estimation of the posterior distribution. Additionally, the method cannot guarantee convergence due to the correlation of samples, which can lead to errors in the estimation. To address these issues, the method can be improved by changing the sampler, increasing the number of samples, or trying different parameterizations of the model. These solutions can lead to more accurate and efficient estimation of the posterior distribution.



# Group Contributions {-}



\newpage 
# Appendix {-}


