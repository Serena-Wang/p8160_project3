---
title: "P8160 - Baysian modeling of hurrican trajectories"
author: "Jiahao Fan, Tianwei Zhao, Yijin Wang, Youlan Shen, Yujia Li"
date: "2023-04-29"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{amsthm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tibble)
```

\newpage 
# Background and Objectives
As hurricanes affect people, the ability to forecast hurricanes is essential for minimizing the risks in suffered areas. Hereby, a hierarchical Bayesian strategy for modeling North Atlantic hurricane counts since 1950 is illustrated. Model integration would be expected to achieve through a Markov chain Monte Carlo algorithm. Contingent on the availability of values for the covariates, the models can be used to explore the seasonality among track data and make predictive inferences on hurricanes.

The data given has 703 observations and following features are recorded for each hurricanes in the North Atlantic: 

* ID: ID of the hurricanes

* Season: In which the hurricane occurred

* Month: In which the hurricane occurred

* Nature: Nature of the hurricane (ET-Extra tropical;DS-Disturbance;NR-Not rated;SS-Sub tropical; TS-Tropical storm)

* time: dates and time of the record

* Latitude and Longitude: The location of a hurricane check point

* Wind.kt: Maximum wind speed (in Knot) at each check point


# Methods
## Baysian Model  
## MCMC Algorithm 

Given the fact that we cannot directly sample from the joint posterior distribution calculated above. We choose to use Gibbs sampling. Gibbs sampling is a sequential sampler that samples from the conditional distribution of each parameter so that the final collection of samples forms a Markov chain whose stationary distribution is the joint distribution that we are interested in. 

In our case, conditional posteriors are more manageable than the joint posterior. Conditional posteriors for each parameter are calculated as the following:

!!! ADD CONDITONAL POSTERIOR

Still, we could not find a regular distribution that has the density function that matches with the conditional posterior of $\sigma^2$. Therefore, we incorporated Metropolis Hasting Random Walk into Gibbs sampling. Metropolis Hasting Random Walk finds approximated values of $\sigma^2$ by accepting and rejecting proposed value with an acceptance probability that helps maintain the detailed balance condition. In our algorithm, we work with log-scaled posterior values instead of the original-scaled posterior because $\frac{1}{\sigma^N}$ dominates the entire posterior value where $N$ is the total number of observations in the data set.

In our investigation of hurricane wind speed, we used Gibbs sampling and Metropolis Hasting Random Walk as the following:

![Gibbs Sampling](algo_fig/gibbs_sampling.png)

![Metropolis Hasting Random Walk](algo_fig/random_walk.png)

\newpage


We decide starting values of each parameters by fitting a linear mixed effects model that is the same as the proposed model in a frequentist way and then extract its corresponding coefficients. Starting values for $\gamma$ are the fixed effects coefficients in the linear mixed effects model.

```{r table1, echo = FALSE, message = FALSE, wanrings = FALSE}
starting_val_df <- tibble(
  terms = c("$\\gamma$", "$\\beta_{i_{1},...,i_{n}}$","$\\sigma^2$", "$\\beta_0$", "$\\mu$", 
            "$A = \\Sigma^{-1}$"
            
                    ),
  starting_val = c("Fixed effect coefficients in the fitted Linear Mixed Effects Model",
                    "Random effect coefficients in the fitted Linear Mixed Effects Model",
                    "Residual Variance in the fitted Linear Mixed Effects Model",
                    "Sum of random effect and fixed effect intercepts in the fitted Linear Mixed Effects Model",
                   "Mean of the $\\beta_{i_{1},...,i_{n}}$ for each predictor",
                   "Mean of the Wishart distribution with $\\nu$ = 5 and scale matrix with 0.7 on the main diagnoal and 0.2 on off-diagnoal"
                    )
)
kable(starting_val_df,
      col.names = c("Terms", "Starting Value"),
      caption = "Starting Values for Gibbs Sampling")
```


With the stating values above, we assume that the true $\sigma^2$ will be around 27 and tried a few values for window size $a$. Finally, with $a = 0.8$, $\sigma^2$ samples converge pretty quickly and the acceptance rate is around 46% with 500 iterations. Therefore, we set $a = 0.8$ in each iteration of Gibbs Sampling and burn the first 400 samples of $\sigma^2$. 

Although our algorithm is not the most efficient algorithm, it has two main advantages: 1) It is simple to compute and does not rely on the calculation of gradient of log densities as Hamiltonian Monte Carlo; 2) By incorporating the Random Walk, we have the flexibility to control the range of sampling area when the conditional distribution of parameter of interest is difficult to simulate from and when our initial prior is not informative .  

## MCMC Results

We ran our algorithm with these starting values for 6000 iterations and decided to burn the first 3000 samples for each parameters for analysis in the rest of this project. We use diagnostic plots to evaluate the quality of these samples. We created trace plots for all 6000 samples for each parameter, and autocorrelation plots and histogram plots for the second half of the samples after burn-in. 

# Results
## Seasonal Analysis 
## Prediction 



# Discussion 



## Group Contributions {-}


\newpage 
# References {-}



\newpage 
# Appendices {-}

## Figures and Tables {-}




















